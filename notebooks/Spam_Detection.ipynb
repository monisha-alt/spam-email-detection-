{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spam Email Detection using NLP and Machine Learning\n",
        "\n",
        "This notebook provides a complete end-to-end analysis for spam email detection, including:\n",
        "- Exploratory Data Analysis\n",
        "- Text Preprocessing\n",
        "- Feature Extraction (TF-IDF)\n",
        "- Model Training and Evaluation\n",
        "- Visualizations\n",
        "- Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data_path = '../data/emails.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"Dataset loaded successfully!\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    df.head()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Dataset not found at {data_path}\")\n",
        "    print(\"Please ensure your dataset is in the data/ directory with columns 'label' and 'text'\")\n",
        "    print(\"\\nCreating sample data for demonstration...\")\n",
        "    # Create sample data for demonstration\n",
        "    sample_data = {\n",
        "        'label': ['spam', 'ham', 'spam', 'ham', 'spam'] * 100,\n",
        "        'text': [\n",
        "            'Congratulations! You won a $1000 lottery. Claim now!',\n",
        "            'Hey, can we meet tomorrow for lunch?',\n",
        "            'URGENT: Click here to claim your prize!',\n",
        "            'Thanks for the meeting today. See you next week.',\n",
        "            'Free money! No deposit required. Click now!'\n",
        "        ] * 100\n",
        "    }\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    print(\"Sample dataset created for demonstration.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check dataset info\n",
        "print(\"Dataset Information:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total samples: {len(df)}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nData types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize labels\n",
        "df['label'] = df['label'].str.lower().str.strip()\n",
        "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
        "\n",
        "# Remove rows with missing values\n",
        "df = df.dropna(subset=['label', 'text'])\n",
        "\n",
        "# Check label distribution\n",
        "print(\"Label Distribution:\")\n",
        "print(\"=\" * 50)\n",
        "label_counts = df['label'].value_counts()\n",
        "print(f\"Spam (1): {label_counts.get(1, 0)}\")\n",
        "print(f\"Ham (0): {label_counts.get(0, 0)}\")\n",
        "print(f\"\\nPercentage:\")\n",
        "print(f\"Spam: {label_counts.get(1, 0) / len(df) * 100:.2f}%\")\n",
        "print(f\"Ham: {label_counts.get(0, 0) / len(df) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize label distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Bar chart\n",
        "label_counts.plot(kind='bar', ax=axes[0], color=['#3498db', '#e74c3c'])\n",
        "axes[0].set_title('Label Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Label', fontsize=12)\n",
        "axes[0].set_ylabel('Count', fontsize=12)\n",
        "axes[0].set_xticklabels(['Ham', 'Spam'], rotation=0)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Pie chart\n",
        "label_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
        "                  colors=['#3498db', '#e74c3c'], startangle=90)\n",
        "axes[1].set_title('Label Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].legend(['Ham', 'Spam'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze text length statistics\n",
        "df['text_length'] = df['text'].str.len()\n",
        "df['word_count'] = df['text'].str.split().str.len()\n",
        "\n",
        "print(\"Text Statistics:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nCharacter Length:\")\n",
        "print(df.groupby('label')['text_length'].describe())\n",
        "print(f\"\\nWord Count:\")\n",
        "print(df.groupby('label')['word_count'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize text length distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Character length distribution\n",
        "spam_lengths = df[df['label'] == 1]['text_length']\n",
        "ham_lengths = df[df['label'] == 0]['text_length']\n",
        "\n",
        "axes[0].hist([ham_lengths, spam_lengths], bins=50, alpha=0.7, \n",
        "             label=['Ham', 'Spam'], color=['#3498db', '#e74c3c'])\n",
        "axes[0].set_title('Text Length Distribution (Characters)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Character Count', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Word count distribution\n",
        "spam_words = df[df['label'] == 1]['word_count']\n",
        "ham_words = df[df['label'] == 0]['word_count']\n",
        "\n",
        "axes[1].hist([ham_words, spam_words], bins=50, alpha=0.7, \n",
        "             label=['Ham', 'Spam'], color=['#3498db', '#e74c3c'])\n",
        "axes[1].set_title('Word Count Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Word Count', fontsize=12)\n",
        "axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample emails from each class\n",
        "print(\"Sample Spam Emails:\")\n",
        "print(\"=\" * 50)\n",
        "spam_samples = df[df['label'] == 1]['text'].head(3)\n",
        "for i, email in enumerate(spam_samples, 1):\n",
        "    print(f\"\\n{i}. {email}\")\n",
        "\n",
        "print(\"\\n\\nSample Ham Emails:\")\n",
        "print(\"=\" * 50)\n",
        "ham_samples = df[df['label'] == 0]['text'].head(3)\n",
        "for i, email in enumerate(ham_samples, 1):\n",
        "    print(f\"\\n{i}. {email}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import preprocessing module\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "from preprocess import TextPreprocessor\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor(use_stemming=False)\n",
        "\n",
        "print(\"Preprocessor initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of preprocessing\n",
        "sample_text = \"Congratulations! You won a $1000 lottery. Visit http://example.com/claim or email us at winner@example.com <html>Click here</html>\"\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Preprocessed Text:\")\n",
        "cleaned = preprocessor.preprocess_text(sample_text)\n",
        "print(cleaned)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess all emails\n",
        "print(\"Preprocessing all emails...\")\n",
        "df['cleaned_text'] = preprocessor.preprocess_batch(df['text'].tolist())\n",
        "print(\"Preprocessing complete!\")\n",
        "print(f\"\\nSample cleaned text:\")\n",
        "print(df[['text', 'cleaned_text']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Extraction (TF-IDF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "max_features = 5000\n",
        "ngram_range = (1, 2)  # Unigrams and bigrams\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=max_features,\n",
        "    ngram_range=ngram_range,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "print(f\"TF-IDF Vectorizer initialized with:\")\n",
        "print(f\"  - Max features: {max_features}\")\n",
        "print(f\"  - N-gram range: {ngram_range}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X = df['cleaned_text']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"\\nTraining set label distribution:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nTest set label distribution:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract TF-IDF features\n",
        "print(\"Extracting TF-IDF features...\")\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Feature extraction complete!\")\n",
        "print(f\"Training features shape: {X_train_features.shape}\")\n",
        "print(f\"Test features shape: {X_test_features.shape}\")\n",
        "print(f\"\\nFeature matrix is sparse: {hasattr(X_train_features, 'toarray')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'SVM': LinearSVC(random_state=42, max_iter=1000),\n",
        "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "}\n",
        "\n",
        "print(\"Models initialized:\")\n",
        "for name, model in models.items():\n",
        "    print(f\"  - {name}: {type(model).__name__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models\n",
        "print(\"Training models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train_features, y_train)\n",
        "    print(f\"{name} training complete!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"All models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_features)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results comparison DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
        "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
        "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
        "    'F1-Score': [results[m]['f1_score'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\" * 60)\n",
        "print(results_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices\n",
        "n_models = len(models)\n",
        "fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
        "\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, (name, result) in enumerate(results.items()):\n",
        "    cm = result['confusion_matrix']\n",
        "    sns.heatmap(\n",
        "        cm, annot=True, fmt='d', cmap='Blues',\n",
        "        xticklabels=['Ham', 'Spam'],\n",
        "        yticklabels=['Ham', 'Spam'],\n",
        "        ax=axes[idx]\n",
        "    )\n",
        "    axes[idx].set_title(f'{name} Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Actual', fontsize=11)\n",
        "    axes[idx].set_xlabel('Predicted', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model comparison bar chart\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "metric_keys = ['accuracy', 'precision', 'recall', 'f1_score']\n",
        "\n",
        "for idx, (metric, key) in enumerate(zip(metrics, metric_keys)):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    models_list = list(results.keys())\n",
        "    values = [results[m][key] for m in models_list]\n",
        "    \n",
        "    bars = ax.bar(models_list, values, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
        "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel(metric, fontsize=11)\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, values):\n",
        "        ax.text(\n",
        "            bar.get_x() + bar.get_width()/2,\n",
        "            bar.get_height() + 0.01,\n",
        "            f'{val:.4f}',\n",
        "            ha='center',\n",
        "            va='bottom',\n",
        "            fontweight='bold'\n",
        "        )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single accuracy comparison chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "models_list = list(results.keys())\n",
        "accuracies = [results[m]['accuracy'] for m in models_list]\n",
        "\n",
        "bars = plt.bar(models_list, accuracies, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Model Comparison - Accuracy Scores', fontsize=14, fontweight='bold')\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width()/2,\n",
        "        bar.get_height() + 0.01,\n",
        "        f'{acc:.4f}',\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontweight='bold'\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best model\n",
        "best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and vectorizer\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save best model\n",
        "model_path = '../models/spam_model.pkl'\n",
        "joblib.dump(best_model, model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Save vectorizer\n",
        "vectorizer_path = '../models/tfidf_vectorizer.pkl'\n",
        "joblib.dump(vectorizer, vectorizer_path)\n",
        "print(f\"Vectorizer saved to {vectorizer_path}\")\n",
        "\n",
        "# Save preprocessor\n",
        "preprocessor_path = '../models/preprocessor.pkl'\n",
        "joblib.dump(preprocessor, preprocessor_path)\n",
        "print(f\"Preprocessor saved to {preprocessor_path}\")\n",
        "\n",
        "print(\"\\nAll components saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Test Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test predictions on sample emails\n",
        "test_emails = [\n",
        "    \"Congratulations! You won a lottery. Claim your prize now!\",\n",
        "    \"Hey, can we meet tomorrow for lunch?\",\n",
        "    \"URGENT: Free money! No deposit required. Click here!\",\n",
        "    \"Thanks for the meeting today. See you next week.\",\n",
        "    \"You've been selected! Claim your $1000 prize immediately!\"\n",
        "]\n",
        "\n",
        "print(\"Testing Predictions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for email in test_emails:\n",
        "    # Preprocess\n",
        "    cleaned = preprocessor.preprocess_text(email)\n",
        "    \n",
        "    # Transform\n",
        "    features = vectorizer.transform([cleaned])\n",
        "    \n",
        "    # Predict\n",
        "    prediction = best_model.predict(features)[0]\n",
        "    label = 'Spam' if prediction == 1 else 'Ham'\n",
        "    \n",
        "    # Get probability if available\n",
        "    if hasattr(best_model, 'predict_proba'):\n",
        "        prob = best_model.predict_proba(features)[0]\n",
        "        spam_prob = prob[1] if len(prob) > 1 else prob[0]\n",
        "    else:\n",
        "        spam_prob = \"N/A\"\n",
        "    \n",
        "    print(f\"\\nEmail: {email}\")\n",
        "    print(f\"Prediction: {label}\")\n",
        "    if spam_prob != \"N/A\":\n",
        "        print(f\"Spam Probability: {spam_prob:.4f}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated a complete spam email detection pipeline:\n",
        "\n",
        "1. **Data Loading**: Loaded and explored the email dataset\n",
        "2. **Preprocessing**: Cleaned text by removing HTML, URLs, emails, punctuation, and applying lemmatization\n",
        "3. **Feature Extraction**: Converted text to TF-IDF features with n-grams\n",
        "4. **Model Training**: Trained three models (Naive Bayes, SVM, Logistic Regression)\n",
        "5. **Evaluation**: Compared models using multiple metrics\n",
        "6. **Visualization**: Created confusion matrices and comparison charts\n",
        "7. **Model Saving**: Saved the best model for production use\n",
        "\n",
        "The system is now ready for deployment!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
